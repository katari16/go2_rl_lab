# HAC-LOCO: Learning Hierarchical Active Compliance Control for Quadruped Locomotion under Continuous External Disturbances

Xiang Zhou*, Xinyu Zhang*Ôºå Qingrui Zhang

Abstract-Despite recent remarkable achievementsin quadruped control, it remains challenging to ensure robust and compliant locomotion in the presence of unforeseen external disturbances.Existing methods prioritize locomotion robustness over compliance, often leading to stiff, high-frequency motions, and energy inefficiency. This paper, therefore,presents a twostage hierarchical learning framework that can learn to take active reactions to external force disturbances based on force estimation.In the first stage,a velocity-tracking policy is trained alongside an auto-encoder to distill historical proprioceptive features.A neural network-based estimator is learned through supervised learning,which estimates body velocity and external forces based on proprioceptive measurements.In the second stage,a compliance action module,inspired by impedance control, is learned based on the pre-trained encoder and policy. This module is employed to actively adjust velocity commands in response to external forces based on real-time force estimates. With the compliance action module,a quadruped robot can robustly handle minor disturbances while appropriately yielding to significant forces,thus striking a balance between robustness and compliance. Simulations and real-world experiments have demonstrated that our method has superior performance in terms of robustness,energy efficiency,and safety.Experiment comparison shows that our method outperforms the state-ofthe-art RL-based locomotion controllers.Ablation studies are given to show the critical roles of the compliance action module.

# I.INTRODUCTION

Quadruped robots are capable of adapting to a wide variety of terrains [1],[2],enabling agile locomotion in complex environments [3]. Recent progress in robot hardware and locomotion control has witnessed the prevalent applications of quadruped robots in diverse industries.Despite the advancements,most existing methods focus on robust locomotion control in unstructured terrains, so they lack compliance with unexpected disturbances [4], [5]. However, a safe and durable robotic system should actively adjust its posture and speed in response to external forces rather than brutally rejecting them,resulting in so-called compliance behavior.

Active compliant locomotion offers distinct advantages over its robust counterpart in terms of running safety, energy efficiency,and human-robot interaction.This preference for compliance over robustness is evident in various animals. For example,horses have evolved the capability to respond compliantly to external disturbances for survival.They possess a sophisticated system for sensing external forces and employ a layered strategy in their responses [6],[7]. When subject to

![](images/2051d5dfd35e868b4a0ead07f9698fe82c7c97f9229db337bdcaf8a26434fdd9.jpg)

![](images/81774c145a41940fd8b1e24c390859e252f9704a01fb42f3452dac13ebb85c9b.jpg)

![](images/5d6c70f78d1035bc9fa3e469c0102170eed0553dfb5740a5d2aa4b845f7386b6.jpg)  
Fig. 1: (a) Animals can respond with a layered strategy by sensing external forces.(b) The proposed HAC-LOCO framework with an active compliance control method.(c) and (d) Versatile compliant behavior is generated by the HAC-LOCO algorithm.

minor disturbances, like a gentle kick,horses leverage spinal reflexes to automatically adjust limb stiffness,effectively absorbing the impact.However, when confronted with persistent forces, such as being guided by a human, horses engage in adjusting their speed and direction accordingly. These adaptive behaviors demonstrate a delicate balance between robustness against minor disruptions and compliance with sustained perturbations.By integrating similar compliance mechanisms into robots,we can enhance their capacity to handle external disturbances,thereby mitigating the risk of damage while improving overall performance.

Despite its advantages, active compliant locomotion poses a significant challenge for quadruped robots navigating various terrains.Existing reinforcement learning (RL)-based control methods,typically tailored for optimizing task-specific rewards,tend to prioritize steady and robust locomotion [1],[8],[9].However,an emphasis on robustness often results in high motion stiffness,causing the robot to exhibit high-frequency movements when faced with unexpected disturbances. These abrupt motions lead to excessive torques that surpass the motor's limits or even lead to damage. Current RL-based controllers also lack the flexibility to adapt to

human intervention,raising the risk of accidents in humanrobot interactions.Hence,the limitation of the state-of-the-art design underscores the need for advancements in control to strike a balance between robust locomotion and compliance behavior,ensuring safe and effective traversal of diverse terrains while maintaining adaptability to external influences.

Traditional compliance control methods, such as impedance and admittance control [1O], aim to characterize the dynamic interactions between a robot and its surroundings to achieve compliance with external forces. These methods typically rely on accurate disturbance estimators to detect external forces. While effective in structured environments, they depend on precise dynamic models and involve intricate design processes, limiting their performance in dynamic and unpredictable settings.In contrast,model-free learning-based approaches, e.g. deep neural network [11],have the capability to directly estimate external disturbances based on historical sensory data, offering greater adaptability in complex environments. Hence,it is possible to emulate the performance of traditional impedance control methods by integrating a disturbance estimation module with a compliance controller through RL. This integration could pave the way for more robust and flexible control strategies that can effectively handle external disturbances in varied and dynamic environments.

In this work, we develop a hierarchical active compliance control framework called HAC-LOCO. This architecture draws inspiration from the layered response strategies of animals to achieve active compliant robot locomotion. Our framework decouples the external force estimation module from the compliance control module and follows the "senseplan-act‚Äù paradigm.HAC-LOCO enables the robot to decide whether to resist or comply with disturbances,offering a more flexible and adaptive solution for compliant locomotion. The main contributions of this paper are threefold:

1ÔºâA hierarchical reinforcement learning framework is pro-posed to achieve active compliance control for quadruped locomotion.This framework results in robust behavior against impact force disturbances and compliant locomotion to persistent external disturbances.Experiments validate that this framework enables the robot to exhibit natural compliance responses to disturbances,enhancing safety, stability,and energy efficiency.   
2ÔºâA learning-based estimator is designed to extract features from historical sensory feedback. This estimator merges an AutoEncoder network with explicit force and velocity estimation networks.Ablation studies demonstrate this estimator's capability to accurately estimate disturbances, allowing the robot to respond promptly to unexpected disruptions.Experimental results showcase that this proposed estimator enhances the robot's compliance, stability, and energy efficiency in complex environments.   
3ÔºâInspired byconventional impedancecontrolÔºåa lightweight learning-based locomotion compliance module has been developed. This module enables the robot to display compliant behaviors in reaction to persistent external disturbances. It is easy to update the compliance module to adjust the robot's impedance without worrying

about retraining the whole HAC-LOCO framework

Hence,this design provides impedance-variable control. allowing for explanation and analysis.

The remainder of this paper is organized as follows. Section II provides a summary of related works on policy adaptation methods for dynamic environments and learningbased compliant control. Section III introduces the proposed hierarchical reinforcement learning architecture,along with its training details. Section IV presents the experimental setup,results,and a comprehensive comparative analysis. Finally, Section V concludes the paper and outlines potential directions for future research.

# II. RELATED WORKS

# A. Policy adaptation for dynamic environments

To actively adapt to dynamic environments,numerous studies focus on training policies that can sense and respond to environmental changes in real-time. One promising approach, known as privileged learning or learning by cheating [12], utilizes a teacher-student architecture to train environmentadaptive policies.In this framework,the student policy is trained under the supervision of a teacher policy that has access to privileged information. Several studies have successfully employed this paradigm to achieve improved performance. Lee et. al. utilized privileged information to facilitate blind locomotion adaptation to challenging terrains [1].RMA leverages privileged learning to enable policies to adapt to varying motor properties and ground friction [13]. PA-LOCO incorporates a multi-encoder architecture designed to process various types of privileged information, such as trunk velocity,external disturbances,and terrain characteristics,thereby preventing the overlap of these information sources and enabling perturbation-adaptive locomotion [14].DreamWaQ introduces a context-assisted estimation network based on variational autoencoders(VAEs),which guides the encoder to extract key features from historical data by predicting future states [15]. The Hybrid Internal Model (HIM) treats the robot's external states as disturbances and uses a hybrid internal embedding to estimate these disturbances.This embedding is optimized through contrastive learning to align closely with the robot's successor state, allowing for a more natural response [16]. Additionally, the concurrent teacher-student framework(CTSÔºâ trains both the teacher and student policies simultaneously within a reinforcement learning paradigm, demonstrating greater robustness and agility in locomotion compared to traditional two-stage privileged learning approaches [17].While these methods enable robots to adapt to dynamic environments with limited proprioceptive feedback,the existing works failed to address the challenge of adapting to persistent external disturbances.

# B. Learning-based compliance control

Many works have focused on achieving compliant locomotion through learning-based methods.Much of the work identifies key factors influencing robot compliance,including reward design, controller parameters,and frequency. Most learning-based approaches integrate action rate penalties,

![](images/cc4d4435b2b84ba53661118ef4701744317ed3f2b1d83b2bd6de03991d8a334f.jpg)  
Fig. 2: HAC-LOCO framework

torque penalties,and energy-effcient reward terms to penalize high torque output and encourage compliant locomotion in response to disturbances [1],[14],[18].Some studies emphasize that a low proportional gain is crucial to increasing robot compliance and thus reducing potential harm to the joint motors [8],[13]. Additionally, Gangapurwala et.al. claims that learning a low-frequency locomotion controller can enhance robot robustness and compliance with external disturbances [19].However, the aforementioned works primarily focus on passive compliance with disturbances.

Some studies specifically address robot compliance for balance recovery in response to external disturbances. Lee et. al.introduced a framework for the simulation and control of humanoid robots that enables physically compliant interactions with the environment [5].However, this architecture requires sensing interaction forces,which makes its realworld implementation complicated. Hartmann et. al. proposed a multi-stage episodic reinforcement learning approach for training a compliant quadruped controller [4]. This approach incorporates explicit recovery stages,where tracking rewards are given irrespective of the agent's motions. This allows the agent to recover compliantly from disturbances before resuming task-specific locomotion,rather than rigidly following predefined commands.However, this work does not account for robots subjected to continuous external forces. Locomotion compliance is significantly affected by a recovery time parameter,but it is challenging to determine an optimal value.Hence,it remains a challenge to make compliant response based on disturbance estimation.

# III. METHODOLOGY

This section presents the HAC-LOCO framework,which consists of two key components (as shown in Fig. 2):(1) a low-level robust locomotion policy that uses a historical feature encoder to accurately estimate body velocity and force disturbances based on proprioceptive feedback,and (2Ôºâa high-level lightweight compliance policy that adjusts velocity commands based on proprioceptive observations and feedback

from the low-level controller, enabling natural compliance to external disturbances.This hierarchical architecture decouples disturbance sensing from compliant behavior generation. It is trained in two phases,with the low-level policy trained first, followed by the high-level policy.

# A. Low-level locomotion policy

1ÔºâPolicy Observation: The low-level locomotion policy observes a historical sequence of proprioceptive information, denoted as oùúá = Ot-H+1,...,Ot]T,with H = 10 in this work.The observation at time step t includes proprioceptive feedback, trunk velocity commands,and the previous policy action,represented as Ot = [St,Ct,√§t-1]T. Specifically, the proprioceptive feedback St consists of angular velocity Wt, the projected gravity vector gt, joint positions and velocities on this step and previous step qt,qt, qt-1, qt-1,and the clock signal tt = [sin(2œÄ ft),cos(2œÄft)] with f = 2.5 (desired step frequency). The velocity command ct is expressed as Ct=lud,ggT,where vand ug denote the horizontal trunk linear velocities command,and œâd is the trunk angular velocity command.

2ÔºâNetwork Architecture:The architecture of the low-level locomotion policy is illustrated in Fig. 2. The encoder module E processes historical proprioceptive observations,encoding them into a low-dimensional feature vector Zt = E(oH).This vector is then passed through two distinct heads: the forcehead fhead, which estimates the external force ft = fhead(Zt), and the velocity-head Uhead,which estimates the body velocity Vt = Uhead(Zt). The outputs of both heads are concatenated to form the latent representation lt = [zt,ft,Vt]T.This latent feature is then passed through a decoder module to predict the next-time-step observation Ot+1 = D(lt).Finally,the concatenated feature lt and the current observation Ot are input into the policy network œÄ,which generates the action at = œÄ(Ot,lt). The output action at E R12 represents target joint positions,which are adjusted and scaled before being tracked by joint PD controllers with gains Kp = 3O and Kd = 0.75.

TABLE I: Reward functions and weights   
![](images/bb6fcbf34580da5c9912b08c897b17d6bf7c197a0629e83ef3979d0a3a7bf62f.jpg)

3Ôºâ Training Process: The low-level locomotion policy is trained using a combination of supervised learning and reinforcement learning.We apply the Proximal Policy Optimization (PPO) algorithm [20] for reinforcement learning. The loss function of PPO is integrated with that of the supervised learning component to update the policy network parameters.Supervised learning facilitates the encoder in extracting critical information about external disturbances. Specifically,the velocity-head and force-head are trained to estimate the trunk velocity and external forces,respectively. A decoder is trained simultaneously to reconstruct the robot's dynamics from the latent representation by the encoder, which encourages the encoder to capture pivotal features from raw historical data.The supervised learning loss consists of two estimation losses and one reconstruction loss.

$$
L _ {\sup } = \omega_ {1} \| \mathbf {v} _ {t} - \hat {\mathbf {v}} _ {t} \| ^ {2} + \omega_ {2} \| \mathbf {f} _ {t} - \hat {\mathbf {f}} _ {t} \| ^ {2} + \omega_ {3} \| \mathbf {o} _ {t + 1} - \hat {\mathbf {o}} _ {t + 1} \| ^ {2}, \tag {1}
$$

where W1, W2,and W3 are weights to balance different loss elements.The total loss for policy training is the sum of the supervised learning loss and the PPO loss,so

$$
L = L _ {\sup } + L _ {\mathrm {P P O}}. \tag {2}
$$

The RL reward function is a weighted sum of task and auxiliary rewards,as shown in Table I.In this table,E and V represent the mean and variance,respectively,and Œ≤i is the duty factor for leg i. The power distribution term, V(Tq)h + V(Tq)t + V(Tq)c, refers to the sum of the joint power variance across the hip,thigh,and calf joints.

An asymmetric actor-critic architecture [21] is used, where the critic network receives a more detailed observation. The critic's input is O = [Ot,Vt,dt,fH,ht]T,with Vt representing the trunk velocity, dt containing environment properties (randomized via domain randomization as described in Section II-C, including ground friction, trunk mass, and motor gain),f =[ft-H+1,..,ft]T representing historical external forces in the trunk frame,and ht being the terrain heightmap surrounding the robot.

# B. High-level compliance policy

1ÔºâPolicy Observation and Action: The high-level compliance policy receives latent information from the encoder of the low-level locomotion policy. In return, it generates a residual velocity command to facilitate compliant behavior in response to external force disturbances.This design follows the philosophy of conventional impedance control that adjusts areference velocity based on external force feedback. To avoid confusion with the low-level policy,we denote the high-level policy's action as at and its observation as Ot. The observation space for the high-level policy includes the robot's proprioceptive feedback St,the original trunk velocity command Ct,the last latent feature lt-1,and the previous actions from both the high-level and low-level policies, at-1 and at-1, respectively. Hence,one has

$$
\mathbf {o} _ {t} ^ {\prime} = \left[ \mathbf {s} _ {t}, \mathbf {c} _ {t}, \mathbf {l} _ {t - 1}, \mathbf {a} _ {t - 1} ^ {\prime}, \mathbf {a} _ {t - 1} \right]. \tag {3}
$$

The action a't represents the residual velocity commands to facilitate compliant behavior, given by at = [‚ñ≥vŒ±,‚ñ≥ud,‚ñ≥œâg]T. The final command to the low-level policy is the sum of the original velocity command and the residual velocity command, expressed as

$$
\mathbf {c} _ {t} ^ {*} = \mathbf {c} _ {t} + \mathbf {a} _ {t} ^ {\prime}, \tag {4}
$$

where c* is the modified velocity command to be tracked by the low-level policy.

2ÔºâTraining Process: The high-level policy is trained in the second stage after the low-level policy is complete. During this phase,the low-level policy is frozen and treated as a part of the environment. The PPO algorithm with an asymmetric actor-critic architecture is applied to train this module.The privileged observation for the critic network,o,is given by o= [O',Vt,dt,fH,ht], where Vt,dt,fH,and ht are defined as in the low-level policy,as described in Section III-A.3.

We make two modifications to the reward function. The first is to revise the velocity tracking rewards in Table I, while the second is to introduce a new resistance-compliance reward. These adjustments to the overall reward function enable a quadruped robot to effectively reject minor force disturbances while exhibiting compliant behavior in response to larger ones.A compliance behavior is represented by a force threshold Œ± and a virtual impedance Œ≤,where Œ± denotes the force threshold that determines the boundary between precise tracking and compliant behavior. When the external force |ft|exceeds this threshold,a robot transitions from precise tracking to compliant behavior. The parameter Œ≤ adjusts compliance flexibility with smaller values increasing compliance and larger values making the robot stiffer. The new velocity tracking rewards are thus adjusted to be

$$
r _ {\text {l i n}, \text {v e l}} = \exp \left(- 4 \| v _ {x y} ^ {d} + a _ {x y} ^ {\prime} - v _ {x y} \| ^ {2}\right), \tag {5}
$$

$$
r _ {\text {a n g . v e l}} = \exp \left(- 4 \| \omega_ {z} ^ {d} + a _ {z} ^ {\prime} - \omega_ {z} \| ^ {2}\right), \tag {6}
$$

It should be noted that the low-level locomotion policy only needs to track the modified velocity commands. The newly

introduced resistance-compliance reward is

$$
r _ {\mathrm {c o m p}} = \left\{ \begin{array}{l l} - \| \mathbf {a} ^ {\prime} \| ^ {2}, & \text {i f} \| \mathbf {f} \| \leq \alpha \\ - \| \mathbf {a} _ {x y} ^ {\prime} - \frac {\mathbf {f} _ {x y}}{\beta} (1 + \frac {\alpha}{| \mathbf {f} _ {x y} |}) \| ^ {2} - \| \mathbf {a} _ {z} ^ {\prime} \| ^ {2}, & \text {i f} \| \mathbf {f} \| > \alpha \end{array} \right.
$$

This formulation encourages the high-level policy to minimize command adjustments when external forces are below the threshold Œ±,and to generate compliant behavior with desired impedanceŒ≤ (i.e,encouragingay fÔºâwhen forces Œ≤ exceed Œ±. It combines the interpretability of model-based control (through the physical meaning of Œ± and Œ≤)with the flexibility of reinforcement learning,optimizing stability and energy effciency while ensuring a smooth transition from precise tracking to compliance.

The learned compliant behavior can be adjusted by chang-ing Œ± and Œ≤ in the reward function without necessitating a retraining of the low-level locomotion policy. This feature facilitates the efficient customization of locomotion controllers with varying characteristics.Additionally, the magnitude of a' is penalized within the resistance-compliance reward to uphold a robot's heading direction in the presence of external disturbances. This reward function can be extended to align a robot's heading with the direction of an external force, catering to specific applications like human-robot interaction. These varied compliant behaviors are showcased in Section IV through experimental validation,underscoring the versatility and effectiveness of the proposed approach.

# C.Implementation details

The high-level policy shares the same implementation setup as the low-level one, including curriculum learning and domain randomization.For curriculum learning,a velocity curriculum and grid-based adaptive terrain curriculum,as proposed in [3],are employed to allow the policy to gradually adapt to increasingly complex commands and environments. The maximum velocity command ranges are set as U* ‚àà [-2,2] m/s,U* ‚àà [-1,1]m/s,and w* ‚àà[-2,2]rad/s The terrain types include smooth slopes, rough slopes, stairs (up and down),and discrete terrain features.

A domain randomization strategy is adopted to mitigate the simulation-to-reality gap,as outlined in Table II. At trainingÔºåwe consider both impact force disturbances and persistent force disturbances. The impact force disturbances are simulated by applying random velocity and angular velocity impulses,‚ñ≥v,‚ñ≥w to a robot every 10 seconds.

TABLE I: Domain Randomization Parameter   
![](images/0a397e8bccd6d9ff638fa6d4058f22cfa81cbcf14f93e9eeb24fae1ec365350b.jpg)

![](images/0d347c35eb6dd22331f3ab2d5fab1b8a1b48d4453e90a7a5f433991c4fb236fb.jpg)  
Fig. 3:A cable-pulley system to generate force disturbances

Persistent external forces f are continuously applied to the trunk of a robot, with their magnitude and direction being randomized every 4 seconds.

For the low-level policy, curriculum-based disturbances are applied, with the maximum disturbance magnitude determined by the curriculum. In contrast, for training the high-level policy, disturbance magnitudes are always set to the upper limits defined by the curriculum.The maximum disturbance magnitudes are set as fllmax = 50 N,|/‚ñ≥vllmax = 1.5 m/s, and |/‚ñ≥œâ|/max = 1.5 rad/s.

# IV.EXPERIMENTAL RESULTS

The training of both high-level and low-level policies is conducted in the Isaac Gym environment with 4O96 parallel agents [22], using an episode length of 2O seconds with early termination triggered when the robot's trunk contacts the ground.The Proximal Policy Optimization (PPO) algorithm follows the hyperparameter setup detailed in [11],and the network architecture is depicted in Fig.2.Experimental validation is performed on the Unitree Gol quadruped robot, with both policies operating at 5O Hz during execution. In the first training stage,approximately 2OO million samples are required,with convergence achieved in around 12 hours on a laptop equipped with an RTX 4060 GPU. The second stage requires only about 2 million samples for full convergence, benefiting from the lightweight nature of the compliance policy and the reusability of the low-level locomotion policy.

We implement the following algorithms for method comparison,ablation studies,and performance evaluation:

¬∑Deep Compliant Control (DCC) [4]: A learning-based compliance control with an additional recovery training.   
¬∑ PA-LOCO [14]: A perturbation-adaptive locomotion controller designed for robust locomotion.   
¬∑ HAC: The purposed HAC-LOCO framework with both high-level and low-level policy.   
¬∑ HAC-Low: Low level robust policy in the purposed HAC-LOCO framework.

# A. Compliant behavior toward continuous external forces

A cable-pulley system,as illustrated in Fig.3,is designed to evaluate the effectiveness of the proposed HAC-LOCO framework in achieving compliant behavior under continuous external disturbances. This system can generate precise, continuous forces, ensuring reproducibility and consistency in

applying force disturbances across trials.The setup consists of two fixed pulleys,a kernmantle rope,and 4.5 L bottled water. The rope transmits the gravitational force exerted by the botted water to the quadruped robot. During the experiment, the water tank is elevated and subsequently released, applying a sustained lateral force to the robot's trunk.

Experiments were conducted in a scenario with a quadruped robot at a stationary condition when it is subject to a lateral impact followed by continuous external forces applied using a cable-pulley system.The compliance behavior of each controller is assessed based on three key performance metrics: maximum joint torque across all joints,trunk roll angle,and robot power consumption. The results,presented in Figure 4,show the performance of each controller under sustained disturbances in terms of stability,safety,and energy efficiency. During the experiment, the DCC controller can withstand the initial impact but fails to maintain stability under continuous disturbances,ultimately leading to a failure to remain upright. This instability is atributed to the control design neglecting sustained disturbances,thereby hindering the robot's ability to adapt its posture for balance maintenance.PA-LOCO shows resilience against instant and continuous external forces, displaying dynamic and aggressive locomotion with rapid leg swings for stabilization. This leads to sharp peaks in joint torque and notably higher power consumption post-impact.

In contrast, the low-level policy in the HAC-LOCO framework effectively mitigates external impacts,resulting in lower maximum joint torques and reduced total power consumption. However, it tends to remain rigid when facing continuous external forces,maintaining a fixed position without postural adaptation. With the aid of the residual velocity commands by a high-level policy based on estimated external forces, HAC-LOCO enhances compliance by dynamically adjusting its position in response to sustained disturbances instead of rigidly resisting them. This approach improves stability in trunk posture with smoother power consumption. Hence, HAC-LOCO shows the highest level of compliance among tested controllers, striking a balance between velocity tracking and compliance optimization.

Comprehensive simulation evaluations are performed to further assess policy performance under instantaneous and sustained disturbances in varied scenarios.Each policy is tested over a 2O-second episode in the Isaac Gym environment with 4,O96 parallel simulations on different setups.Random impulsive forces are applied every 3 seconds for impact force assessment,while sustained external forces were randomly reapplied at the same intervals for continuous force evaluation. The magnitudes of external forces and domain randomization settings are kept consistent with the training setup.The findings from the simulations are outlined in Table III.

The simulation results reveal that the HAC-Low policy attains the highest survival rate, demonstrating sufficient robustness against external disturbances.Its performance under instantaneous forces closely matches that of HAC, the full HAC-LOCO framework, suggesting that the lowlevel policy primarily governs responses to sudden impacts. However, in the face of continuous disturbances,the complete

![](images/2b2bb003a0cfb628ffe513645725756c0db40d5ea0adb2c12daf80fc6b238b12.jpg)

![](images/186cae1e21e00efbed4878585fe8689af9b4255baae6cc288bfd536e6c23b572.jpg)

![](images/6be2db57dae7b7719b4194618f7ecf41dbde4c4c0051aad857f5165a80ed7f7a.jpg)  
Fig. 4: Compliant performance of different controllers at impact and persistent lateral forces.

HAC-LOCO framework displays notably lower maximum torque compared to the low-level policy in isolation. This decrease affrms that the high-level policy effectively bolsters compliance when confronted with ongoing external forces. These outcomes align with those observed in physical trials, further affirming the efficacy of HAC-LOCO in enhancing compliant locomotion.

TABLE II: Simulation Results of Different Controllers   
![](images/f50f3da23b946b58dba8de561de68bb6e8cf316738b5083745954a167ceb2d79.jpg)

# B. Effectiveness of Low Level Network Design

A series of ablation studies are conducted in a simulated dynamic environment to validate the effectiveness of different modules of the low-level network design. Specifically, the full low-level motion policy (L-P) is compared against three modified versions: L-P without velocity estimation (L-P w/o vel-est),L-P without force estimation (L-P w/o force-est), and L-P without both force and velocity estimation (L-P w/o force-vel-est). These variants are trained by setting the corresponding supervised learning loss terms to zero while maintaining an identical number of training steps.

![](images/018552d5c133fd768c872e5e103f74af661f3fbb4e9de2407b9860cf87e7df96.jpg)  
Fig. 5: Comparison of velocity tracking performance with and without explicit estimation of external forces and velocity.

The evaluation is performed in a test environment where the robot tracks a forward velocity of 1.5 m/s on rough terrain. At t = 2 s,a forward impact force equivalent to an instant velocity change of 2.5 m/s is applied, followed by a backward continuous force of 3O N from t = 5 s to t = 8 s. The velocity tracking results are shown in Fig. 5.Experimental results demonstrate that the full low-level motion policy (L-P) achieves the highest velocity tracking accuracy,maintaining minimal estimation errors for both velocity and external force.When velocity estimation is removed (L-P w/o vel-est), tracking accuracy deteriorates after the impact. Similarly,removing force estimation (L-P w/o force-est) leads to reduced tracking accuracy under sustained forces.The most significant degradation occurs in the absence of both force and velocity estimation (L-P w/o vel-est & forceest),where tracking performance degradation greatly under continuous disturbances.These findings suggest that explicit force and velocity estimation contribute to enhancing velocity tracking performance in environments with both instant and continuous disturbances.

# C. Effectiveness of High Level Network

To assess the effectiveness of the high-level policy, we train two separate high-level policies with different compliance parameters:Œ± = 10,Œ≤ = 10 (low impedance) and Œ± = 10,Œ≤ = 3O (high impedance). These configurations allow us to analyze the influence of varied impedance on the robot's compliance behavior,where lower Œ≤ correspond to higher compliance,making the robot more responsive to external forces,while higher Œ≤ values result in a stiffer response.

To assess the compliance characteristics,the robot is commanded to track a forward velocity of 1m/s and an angular velocity of O.5rad/s on a rough flat surface. At t = 5 s,a sustained external force f = [15,O,O]N is applied in the world frame,and at t = 1O, the external force changed to [30, 0, O] N. The estimated external force and the output of the command correction module are shown in Fig. 6.To ensure consistent motion states across evaluations,the correction

![](images/e387de455c715dcb176af3cc9d977c1be6d996f70c0644cf7b28d496abda11ad.jpg)

![](images/bb1b8ca32121ea98ec077faae7565e072ca63deadb4b7646bcc8ddbda8ae904d.jpg)  
Fig.6: High-level policy output in response to external forces applied to the robot's trunk.The correction output is approximately proportional to the external force, demonstrating impedance-like behavior. By tuning the compliance parameters Œ± and Œ≤ in the reward function during training, the virtual impedance characteristics of the policy can be modulated, enabling adaptive compliance control.

output is recorded but not fed into the low-level motion network.

The results indicate that although the external force is timevarying in the robot's body frame due to motion dynamics, the high-level policy is able to estimate the applied force with reasonable accuracy,though a minor delay is observed at the instant of the force application. The correction output from the high-level policy aligns with the direction of the external force and is approximately proportional in magnitude, suggesting that the high-level network effectively runs as impedance control.Under identical motion and external force conditions, a higher value of Œ≤ leads to a smaller correction output, implying a lower compliance level. Hence,by tuning Œ≤, the robot can achieve diferent impedance characteristics, offering a clear physical interpretation of the compliance parameters. These findings confirm that the high-level command correction module effectively regulates compliance behavior, allowing tunable and interpretable impedance control for quadruped locomotion.

# D. Locomotion in Various Outdoor Environments with Different Compliance

As shown in Fig.7, the proposed HAC-LOCO framework is validated in diverse outdoor environments,showcasing its adaptability to different compliance behaviors.The robot is initially tested on grassland,where controllers with higher impedance enable it to tow a picnic cart efficiently,while controllers with lower impedance allow a human operator to drag the quadruped and guide its heading direction with ease.

![](images/585b1727ce07165bfdd82a7919d7a09d484be4a3f34a2656c75aa22c8a4abd76.jpg)  
Fig. 7: Outdoor experiments demonstrating the versatility of HAC-LOCO on different terrains.

The quadruped is further evaluated on stair terrain,where a high-impedance policy facilitates stable locomotion while dragging a large water bottle upstairs. Meanwhile,a lowerimpedance policy enables the robot to follow human guidance when ascending and descending stairs.These scenarios validate the robustness of the HAC-LOCO low-level policy in handling various terrains and external disturbances,as well as the versatility of the high-level policy in achieving diverse compliant behaviors suitable for real-world applications.

# V. CONCLUSIONS

This paper presented HAC-LOCO,a hierarchical reinforcement learning framework for active compliance control in quadruped locomotion. By integrating a robust low-level motion policy with a high-level compliance module,the framework enables the robot to resist impact forces while exhibiting compliant responses to sustained external disturbances.A learning-based estimator combining an AutoEncoder with explicit force and velocity estimation significantly improved disturbance estimation accuracy, enhancing velocity tracking and stability. Additionally, the lightweight compliance module, inspired by impedance control,allows for dynamic compliance adjustments without retraining the entire framework. Extensive simulations and physical experiments demonstrated that HAC-LOCO achieves superior compliance, reducing peak joint torques and power consumption while maintaining stability under external forces. The ability to modify compliance characteristics by adjusting parameters further highlights its adaptability. Future work will focus on adaptively selecting the force threshold and virtual impedance based on sensory feedback,improving the robot's intelligence for actively responding to disturbances in unstructured environments.

# REFERENCES

[1] J.Lee,J.Hwangbo,L.Wellhausen, V. Koltun,and M. Hutter,‚ÄúLearning quadrupedal locomotion over challenging terrain,‚ÄùScience Robotics, vol.5,no.47,p.eabc5986,2020.   
[2] S.Choi,G.Ji,J.Park,H. Kim,J. Mun,J.H. Lee,and J.Hwangbo, ‚ÄúLearning quadrupedal locomotion on deformable terrain,‚ÄùScience Robotics,vol.8,no.74,p.eade2256,2023.   
[3]G.B.Margolis,G. Yang,K.Paigwar, T.Chen,and P.Agrawal,‚ÄúRapid locomotion via reinforcement learning,The International Journal of Robotics Research,vol.43,no.4,pp.572-587,2024.   
[4] A.Hartmann,D.Kang,F. Zargarbashi,M. Zamora,and S. Coros, ‚ÄúDeep compliant control for legged robots,‚Äùin 2024 IEEE International Conference on Robotics and Automation (ICRA),2024, pp.11 421- 11 427.

[5] S.Lee,P. S.Chang,and J.Lee,‚ÄúDeep compliant control,‚Äùin ACM SIGGRAPH 2022 Conference Proceedings,ser. SIGGRAPH'22ÔºéNew York,NY, USA:Association for Computing Machinery,2022.   
[6] S.Morton and A.Bastian,‚ÄúCerebellar contributions to locomotor adaptations during splitbelt treadmill walking,‚Äù Journal of Neuroscience, vol.26,no.36,pp.9107-9116,Sept.2006.   
[7] J.Merel,M. Botvinick,and G.Wayne,‚ÄúHierarchical motor control in mammals and machines‚Äù Nature communications, vol.10, no.1, p. 5489,2019.   
[8] J.Hwangbo,J.Lee,A.Dosovitskiy,D.Bellicoso,V.Tsounis,V.Koltun, and M.Hutter,‚ÄúLearning agile and dynamic motor skills for legged robots,Science Robotics,vol.4,no.26,p.eaau5872,2019.   
[9]G.B.Margolis and P.Agrawal,‚ÄúWalk These Ways: Tuning robot control for generalization with multiplicity of behavior,‚Äôin Proceedings of The 6th Conference on Robot LearningÔºåser.Proceedings of Machine Learning Research,K.Liu,D.Kulic,and J.Ichnowski, Eds., vol.205ÔºéPMLR,14-18 Dec 2023,pp.22-31.[Online].Available: https://proceedings.mlr.press/v205/margolis23a.html   
[10] T. Zhang,X.Peng,F.Lin,X.Xiong,and Y.Lou,‚ÄúWhole-body compliance control for quadruped manipulator with actuation saturation of joint torque and ground friction,‚Äùin 2O24 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),2024,pp.11124- 11131.   
[11] X.ZhangÔºåZ.Xiao,X. Zhou,and Q. ZhangÔºå‚ÄúSYNLOCO-VE: Synthesizing central pattrn generator with reinforcement learning and velocity estimator for quadruped locomotion,‚ÄùOptimal Control Applications and Methods,pp.1-9,2024.   
[12] D. Chen,B. Zhou,V. Koltun,and P. Krahenbuhl,‚ÄúLearning by cheating,in Proceedings of the Conference on Robot Learning,ser. Proceedings of Machine Learning Research,L.P.Kaelbling, D.Kragic, and K.Sugiura,Eds.,vol.100.PMLR,30 Oct-01 Nov 2020, pp. 66-75.   
[13] A.Kumar,Z.Fu,D.Pathak,and J.Malik,‚ÄúRMA:Rapid Motor Adaptation forLegged Robots,‚Äôin Proceedingsof Robotics:Science and Systems,Virtual, July 2021.   
[14] Z. Xiao, X. Zhang,X. Zhou,and Q. Zhang,‚ÄúPA-LOCO: Learning perturbation-adaptive locomotion for quadruped robots,‚Äùin 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),2024,pp.9110-9115.   
[15] I.M. Aswin Nahrendra,B.Yu,and H. Myung,‚ÄúDreamwaq: Learning robust quadrupedal locomotion with implicit terrain imagination via deep reinforcement learning,‚Äùin 2O23 IEEE International Conference on Robotics and Automation (ICRA),2023,pp.5078-5084.   
[16]J. Long,Z.Wang,Q.Li,L.Cao,J.Gao,and J.Pang,‚ÄúHybrid internal model: Learning agile legged locomotion with simulated robot response," in The Twelfth International Conference on Learning Representations, 2024.   
[17]H.Wang,H.Luo,W. Zhang,and H.Chen,‚ÄúCTS:Concurrent teacherstudent reinforcement learning for legged locomotion,‚ÄùIEEE Robotics and Automation Letters,vol.9,no.11,pp.9191-9198,2024.   
[18]G. Ji, J.Mun,H. Kim,and J.Hwangbo,‚ÄúConcurrent training of a control policy and a state estimator for dynamic and robust legged locomotion‚ÄùIEEE RoboticsandAutomation Letters,vol.7,no.2,pp. 4630-4637,2022.   
[19] S.Gangapurwala,L.Campanaro,and I.Havoutis,‚ÄúLearning lowfrequency motion control for robust and dynamic robot locomotion," in 2023 IEEE International Conference on Robotics and Automation (ICRA),2023,pp.5085-5091.   
[20] J. Schulman,F.Wolski,P.Dhariwal,A.Radford,and O.Klimov, ‚ÄúProximal policy optimization algorithms,‚Äù2017.   
[21]L.Pinto,M.Andrychowicz,P.Welinder,W. Zaremba,and P.Abbeel, ‚ÄúAsymmetric actor critic for image-based robot learning,‚Äôin Proceedings ofRobotics:Science and Systems,Pittsburgh,Pennsylvania,June 2018.   
[22]V.Makoviychuk,L.Wawrzyniak,Y. Guo,M.Lu,K. Storey,M.Macklin, D. Hoeller, N. Rudin, A. Allshire,A. Handa,and G.State,‚ÄúIsaac gym: High performance gpu-based physics simulation for robot learning," 2021.
